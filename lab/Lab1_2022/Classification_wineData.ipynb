{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models for Classification: Wine Dataset\n",
    "\n",
    "## IMPORTANT: make sure to rerun all the code from the beginning to obtain the results for the final version of your notebook (this is the way we will do it for evaluating your HWs!)\n",
    "\n",
    "### Dataset description\n",
    "\n",
    "We will be working with a dataset on wines from the UCI machine learning repository\n",
    "(http://archive.ics.uci.edu/ml/datasets/Wine). It contains data for 178 instances. \n",
    "The dataset is the results of a chemical analysis of wines grown in the same region\n",
    "in Italy but derived from three different cultivars. The analysis determined the\n",
    "quantities of 13 constituents found in each of the three types of wines. \n",
    "\n",
    "### The features in the dataset are:\n",
    "\n",
    "- Alcohol\n",
    "- Malic acid\n",
    "- Ash\n",
    "- Alcalinity of ash\n",
    "- Magnesium\n",
    "- Total phenols\n",
    "- Flavanoids\n",
    "- Nonflavanoid phenols\n",
    "- Proanthocyanins\n",
    "- Color intensity\n",
    "- Hue\n",
    "- OD280/OD315 of diluted wines\n",
    "-Proline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the dataset we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.2\n"
     ]
    }
   ],
   "source": [
    "#let's import the sklearn library\n",
    "import sklearn\n",
    "\n",
    "#let's print out the version of scikit-learn\n",
    "print(sklearn.__version__)\n",
    "\n",
    "#this imports the datasets module, which has useful datasets\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the dataset from scikit learn\n",
    "wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the description of the dataset from the scikit learn documentation: https://scikit-learn.org/0.23/modules/classes.html#module-sklearn.datasets\n",
    "\n",
    "(**Note**: we are considering the scikit-learn version that is installed in the labs Te and Ue, but there are more recent ones)\n",
    "\n",
    "Now let's understand a little bit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
      " ...\n",
      " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
      " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
      " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n",
      "(178, 13)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "['class_0' 'class_1' 'class_2']\n",
      ".. _wine_dataset:\n",
      "\n",
      "Wine recognition dataset\n",
      "------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 178 (50 in each of three classes)\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- Alcohol\n",
      " \t\t- Malic acid\n",
      " \t\t- Ash\n",
      "\t\t- Alcalinity of ash  \n",
      " \t\t- Magnesium\n",
      "\t\t- Total phenols\n",
      " \t\t- Flavanoids\n",
      " \t\t- Nonflavanoid phenols\n",
      " \t\t- Proanthocyanins\n",
      "\t\t- Color intensity\n",
      " \t\t- Hue\n",
      " \t\t- OD280/OD315 of diluted wines\n",
      " \t\t- Proline\n",
      "\n",
      "    - class:\n",
      "            - class_0\n",
      "            - class_1\n",
      "            - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  Comparison of Classifiers in High Dimensional Settings, \n",
      "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Technometrics). \n",
      "\n",
      "  The data was used with many others for comparing various \n",
      "  classifiers. The classes are separable, though only RDA \n",
      "  has achieved 100% correct classification. \n",
      "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "  (All results using the leave-one-out technique) \n",
      "\n",
      "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Journal of Chemometrics).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's print the data matrix\n",
    "print(wine.data)\n",
    "\n",
    "#let's print the dimension of the data matrix\n",
    "print(wine.data.shape)\n",
    "\n",
    "#let's print the target (labels)\n",
    "print(wine.target)\n",
    "\n",
    "#let's print the features names\n",
    "print(wine.feature_names)\n",
    "\n",
    "#let's print the targets names names\n",
    "print(wine.target_names)\n",
    "\n",
    "#let's print the description of the dataset\n",
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify a bit the problem (and the presentation), we are going to classify class \"1\" vs the other two classes (0 and 2). We are going to relabel the other classes (0 and 2) as \"-1\".\n",
    "\n",
    "For convenience, let's save the instances (vectors of features) in matrix $\\mathbf{X}$ and the targets into a vector $\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix of instances\n",
      "[[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
      " ...\n",
      " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
      " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
      " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n",
      "Vector of labels\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "X = wine.data\n",
    "Y = wine.target\n",
    "\n",
    "#let's print out the matrix of instances and the vector of targets, just to make sure that everything looks ok\n",
    "print(\"Matrix of instances\")\n",
    "print(X)\n",
    "\n",
    "print(\"Vector of labels\")\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's relabel the labels for classes 0 and 2 as stated before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "#let's relabel classes 0 and 2 as -1\n",
    "for i in range(len(Y)):\n",
    "    if Y[i] != 1:\n",
    "        Y[i] = -1\n",
    "        \n",
    "#let's print the new vector Y\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Split into Training and Testing ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we actually learn the model, it is important that we perform two operations:\n",
    "1. split the data into a training set and a test set\n",
    "2. normalize the features\n",
    "\n",
    "**Note**: some of there operations can be done with scikit-learn functions, but we do them \"manually\" to get a better understanding of what is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to split the data into training and testing. Let's say we keep 80% of the data for training and 20% for testing. How do we split the data?\n",
    "\n",
    "What about keeping the first 80% of the raws for training and the last 20% of rows for testing? Is it a good idea?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: randomly permute the rows, and then split as suggested before.\n",
    "\n",
    "**Note**: since we use randomization in some parts, let's fix a random seed (so that the entire procedure is reproducible, hopefully)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1 -1  1 -1 -1  1 -1 -1  1 -1 -1  1 -1  1  1 -1 -1  1 -1 -1  1 -1  1\n",
      " -1 -1 -1 -1 -1 -1  1 -1 -1 -1  1  1  1  1  1 -1 -1  1 -1 -1 -1 -1  1 -1\n",
      " -1 -1  1 -1 -1 -1  1  1 -1  1 -1  1  1 -1 -1  1 -1 -1  1 -1  1 -1 -1 -1\n",
      " -1 -1  1  1 -1 -1 -1  1 -1 -1  1 -1 -1  1  1 -1  1  1  1 -1  1  1  1 -1\n",
      "  1  1  1  1 -1 -1 -1 -1 -1  1 -1 -1 -1  1 -1  1  1 -1 -1  1  1 -1 -1  1\n",
      "  1  1 -1 -1  1 -1  1 -1 -1  1  1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1  1  1\n",
      " -1 -1  1  1 -1 -1 -1  1  1 -1 -1 -1  1  1  1 -1 -1 -1 -1 -1 -1  1 -1 -1\n",
      " -1  1  1 -1  1 -1 -1  1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "# we need to import numpy\n",
    "import numpy as np\n",
    "\n",
    "# set the random seed to your ID number\n",
    "IDnumber = 1\n",
    "np.random.seed(IDnumber)\n",
    "\n",
    "#let's generate a permutation among the number of rows\n",
    "m = wine.data.shape[0]\n",
    "permutation = np.random.permutation(m)\n",
    "\n",
    "X_perm = X[permutation]\n",
    "Y_perm = Y[permutation]\n",
    "\n",
    "#let's print Y_perm\n",
    "print(Y_perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data and save into 2 new data matrices/vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n",
      "89\n",
      "[ 1  1 -1  1  1  1 -1  1  1  1  1 -1 -1 -1 -1 -1  1 -1 -1 -1  1 -1  1  1\n",
      " -1 -1  1  1 -1 -1  1  1  1 -1 -1  1 -1  1 -1 -1  1  1 -1 -1  1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1  1  1 -1 -1  1  1 -1 -1 -1  1  1 -1 -1 -1  1  1  1 -1 -1\n",
      " -1 -1 -1 -1  1 -1 -1 -1  1  1 -1  1 -1 -1  1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "fraction_train = 0.5\n",
    "fraction_test = 1- fraction_train\n",
    "\n",
    "m_training = int(X_perm.shape[0] * fraction_train)\n",
    "print(m_training)\n",
    "\n",
    "m_test = int(X_perm.shape[0] - m_training)\n",
    "print(m_test)\n",
    "\n",
    "X_training = X_perm[:m_training,:]\n",
    "Y_training = Y_perm[:m_training]\n",
    "\n",
    "X_test = X_perm[m_training:,:]\n",
    "Y_test = Y_perm[m_training:]\n",
    "\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now center and scale the data to have unit variance. This is an important step for the stability of the computation and for other reasons. We are going to use the standard scaler from scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.75978906  0.90435899  0.63672355 ...  0.02998213 -1.19559091\n",
      "  -0.25927744]\n",
      " [-0.77075732 -0.63309317 -0.71528317 ...  0.49486179  0.45548645\n",
      "  -1.26662133]\n",
      " [ 0.69953133  0.75527272  0.71398107 ...  0.02998213  1.03481184\n",
      "   0.23687701]\n",
      " ...\n",
      " [-0.83101506 -1.00580884  0.71398107 ...  1.09920535 -0.50040044\n",
      "  -0.26529144]\n",
      " [ 0.60311896 -0.52127847  0.32769344 ...  0.12295806  0.55686839\n",
      "   1.06380109]\n",
      " [-0.92742743  0.82049796 -0.63802564 ...  2.16842857  0.2672057\n",
      "  -1.08319453]]\n"
     ]
    }
   ],
   "source": [
    "#load the StandardScaler module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we first \"learn\" the scaler function using the training data\n",
    "scaler = StandardScaler().fit(X_training)\n",
    "\n",
    "# we then apply the scaling function to both training and test data, since we want to simulate what happens when we have data for training and we have future data\n",
    "X_train_scaled = scaler.transform(X_training)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#let's print the scaled version of X_traing_scaled\n",
    "print(X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a Model ##\n",
    "\n",
    "We now need to decide which model/algorithm we are going to use for our classification task. There are several models available in scikit-learn: https://scikit-learn.org/0.23/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start from the simplest models, that is, linear models: https://scikit-learn.org/0.23/modules/classes.html#module-sklearn.linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we find the best hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define a loss function and then use Empirical Risk Minimization (ERM). \n",
    "\n",
    "What loss function does it make sense to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is the actual algorithm? We are going to consider the **Perceptron** algorithm: https://scikit-learn.org/0.23/modules/generated/sklearn.linear_model.Perceptron.html\n",
    "\n",
    "Let's load the corresponding module in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's us the Perceptron algorithm as implemented in scikit-learn. It proceeds in iterations.\n",
    "\n",
    "The Perceptron has several parameters, some of which we will understand later on. An important one is $\\texttt{tol}$, that represents how much the training error should improve in one iteration for the algorithm to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(random_state=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's learn a model using Perceptron\n",
    "\n",
    "#we first define the classifier, fixing the random state for reproducibility\n",
    "perceptron_classifier = Perceptron(random_state=IDnumber, tol=1e-3)\n",
    "\n",
    "#let's now learn the classifier (i.e., run the perceptron to fix the weights)\n",
    "perceptron_classifier.fit(X_train_scaled, Y_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the model we learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of features (vector w):\n",
      "[[-4.96848696 -1.74558664 -4.09074263  4.03186637 -1.72682244  0.3485182\n",
      "   2.99148804 -0.34645642  2.28746151 -6.78434457  3.53914454  1.1485614\n",
      "  -6.59931033]]\n",
      "Bias of the model (value b):\n",
      "[-6.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Coefficient of features (vector w):\")\n",
    "print(perceptron_classifier.coef_)\n",
    "\n",
    "print(\"Bias of the model (value b):\")\n",
    "print(perceptron_classifier.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does our method perform?\n",
    "\n",
    "We need to compute the training error of the hypothesis $h_S$ we learned from the training set $S$. There is no function in python to compute the training error $L_S(h_S)$. However, there is a function to compute the \\emph{score}, that for the 0-1 loss corresponds to $1 - L_S(h_S)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.0\n"
     ]
    }
   ],
   "source": [
    "#let's compute the training error\n",
    "training_error = 1. - perceptron_classifier.score(X_train_scaled, Y_training)\n",
    "\n",
    "#let's print the training error\n",
    "print(\"Training error:\", training_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we don't care about the training error... we are interested in the generalization error! How do we estimate it? Let's use some data that we did not use for training, that is what we called test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test error:  0.0449438202247191\n"
     ]
    }
   ],
   "source": [
    "#let's compute the test error\n",
    "test_error = 1. -perceptron_classifier.score(X_test_scaled, Y_test)\n",
    "\n",
    "#let's print the test error\n",
    "print(\"Test error: \", test_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of the amount of training data ##\n",
    "\n",
    "We will now try to understand the impact of the amount of data we have for training.\n",
    "\n",
    "To do this, we are going to train a model using a subset of the data with $10*i$ samples, for $i=1,2,3,\\dots,10$, and then compute the training error and the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total number of samples, useful for later on\n",
    "m_total = X.shape[0]\n",
    "\n",
    "#two lists where to save the training error and the test error, useful for plotting\n",
    "train_errors = list()\n",
    "test_errors = list()\n",
    "\n",
    "#let's define the learner we use in this part\n",
    "perceptron_class = Perceptron(random_state = IDnumber, tol=1e-3 )\n",
    "\n",
    "for i in range(1,10):\n",
    "    # we now repeat all the previous steps\n",
    "    # split into training and test\n",
    "\n",
    "    \n",
    "    #scale the data according to the training test, for both training and testing\n",
    "\n",
    "    \n",
    "    #let's now learn the classifier (i.e., run the perceptron to fix the weights\n",
    "\n",
    "print(train_errors)\n",
    "print(test_errors)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the training and test error as a function of the training dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following is to have the plots appearing inline\n",
    "%matplotlib inline\n",
    "\n",
    "#import the pyplot module from matplotlib for plotting (functions are similar to matlab)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_axis=range(10,100,10)\n",
    "plt.plot(x_axis,train_errors,'x:')\n",
    "plt.plot(x_axis,test_errors,'o:')\n",
    "plt.legend([\"Training error\",\"Test error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Impact of initial conditions by the perceptron \n",
    "\n",
    "Note that the solution found by the Perceptron algorithm depends on the initial condition. Let's learn a model with a different random seed for the Perceptron and see how different the model is from the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's learn a new model using Perceptron\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of normalization\n",
    "\n",
    "Let's try to understand what the impact of scaling data is. Let's learn a model without without normalizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's learn a new model using Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6130dc0ca154d48d4309febcf6869dce2f08df7913a461d2bad8c19ec3dd616"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
